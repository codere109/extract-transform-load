{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e272c38d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This script was created to perform the ETL process for building a database out of D&D and folklore creatures. The ETL process was executed with the use of Python libraries and PostgreSQL. The details of each step are explained throughout this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28482f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy.dialects import postgresql\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5007c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the PostgreSQL confidential values\n",
    "from config import postgresql_key, postgresql_port, postgresql_host, postgresql_db, postgresql_user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40f9a7f",
   "metadata": {},
   "source": [
    "## Extract\n",
    "\n",
    "#### Web Scraping\n",
    "The first data source was scraped from a [Wikipedia multi-page table](https://en.wikipedia.org/wiki/List_of_legendary_creatures_(A)) that alphabetically lists folklore/mythological creatures along with their origin and a short description. Due to the nature of this algorithm, we decided it was most efficient to format the data as it was scraped. We encountered issues where the list items had inconsistent formatting with their anchor elements so we had to resort to pulling the displayed text then delimiting it by parentheses in order to still get the desired information. After storing this data into string variables, we had to convert any special and accented characters to standard letters. We used the ``strip_accents`` function to this effect. After the data was properly formatted it was stored in a DataFrame for later use.\n",
    "\n",
    "#### CSV\n",
    "The second data source was a CSV downloaded from [Kaggle](https://www.kaggle.com/datasets/mrpantherson/dnd-5e-monsters). We imported the CSV using Pandas native ``read_csv`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8edff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes accented characters from a string\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# set up the splinter service\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless = True)\n",
    "\n",
    "# specify the initial web browser URL\n",
    "web_url = \"https://en.wikipedia.org/wiki/List_of_legendary_creatures_(A)\"\n",
    "\n",
    "# send the browser instance to the provided URL\n",
    "browser.visit(web_url)\n",
    "\n",
    "# initialize the destination lists\n",
    "names = []\n",
    "origins = []\n",
    "descriptions = []\n",
    "\n",
    "# define the alphabetical list for browser navigation\n",
    "alphabet = [\"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"A\"]\n",
    "\n",
    "# iterate through the web pages and scrape the content into the destination lists\n",
    "for letter in alphabet:\n",
    "    \n",
    "    # define the BeautifulSoup instance\n",
    "    soup = bs(browser.html, \"html.parser\")\n",
    "    \n",
    "    # retrieve the creature list\n",
    "    creatures = soup.body.find(\"div\", class_ = \"mw-body-content mw-content-ltr\").find_all(\"ul\")[1].find_all(\"li\")\n",
    "    \n",
    "    # split the list items into name and origin then store into lists\n",
    "    for creature in creatures:\n",
    "        \n",
    "        # store the whole list item text\n",
    "        myStr = creature.text\n",
    "        \n",
    "        # make sure there are parentheses to delimit with\n",
    "        if \"(\" in myStr:\n",
    "            \n",
    "            # split the text by parentheses\n",
    "            split0 = myStr.split(\"(\")\n",
    "            split1 = split0[1].split(\")\")\n",
    "            \n",
    "            # extract the relevant information\n",
    "            name = strip_accents(split0[0].strip()).lower()\n",
    "            origin = strip_accents(split1[0].strip()).lower()\n",
    "            description = strip_accents(split1[len(split1) - 1].strip()[1:].strip()).lower()\n",
    "            \n",
    "            # store the information into lists\n",
    "            names.append(name)\n",
    "            origins.append(origin)\n",
    "            descriptions.append(description)\n",
    "    \n",
    "    # advance to the next page\n",
    "    browser.links.find_by_href(f\"/wiki/List_of_legendary_creatures_({letter})\").click()\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# store the lists in a dataframe\n",
    "folklore_creatures_df = pd.DataFrame({\n",
    "    \"names\": names,\n",
    "    \"origins\": origins,\n",
    "    \"descriptions\": descriptions})\n",
    "\n",
    "# import dnd monster csv into a Pandas dataframe\n",
    "csv_file = \"Resources/dnd_monsters.csv\"\n",
    "dndmonster_df= pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63a83fd",
   "metadata": {},
   "source": [
    "## Transform\n",
    "\n",
    "#### Web Scraping\n",
    "This transformation process was completed within the web scraping algorithm for better efficiency.\n",
    "\n",
    "#### CSV\n",
    "This dataset contained more data than we needed so we decided to remove some columns after importing the CSV into a DataFrame. We also encountered some invalid data so some rows had to be dropped. The \"cr\" column contained data in the incorrect format so we had to perform some extra steps to replace ``string`` fractions into ``string`` decimals then convert the whole column into the ``float`` type. After we had all the data we wanted into the correct format, we renamed the columns to better represent the data they contain. We then stored this transformed data into a new DataFrame for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cfe750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim out unnecessary columns\n",
    "clean_monster_df = dndmonster_df[[\"name\",\"cr\",\"type\",\"size\",\"ac\",\"hp\",\"align\"]]\n",
    "\n",
    "# remove invalid data\n",
    "clean_monster_df.dropna()\n",
    "\n",
    "# convert text representations of fractions into decimals\n",
    "clean_monster_df[\"cr\"] = clean_monster_df[\"cr\"].apply(lambda s: re.sub(r\"1/4\",\"0.25\", str(s)))\n",
    "clean_monster_df[\"cr\"] = clean_monster_df[\"cr\"].apply(lambda s: re.sub(r\"1/2\",\"0.50\", str(s)))\n",
    "clean_monster_df[\"cr\"] = clean_monster_df[\"cr\"].apply(lambda s: re.sub(r\"1/8\",\"0.125\", str(s)))\n",
    "\n",
    "# convert 'challenge rating' column into float from string\n",
    "clean_monster_df[\"cr\"] = clean_monster_df[\"cr\"].astype(float)\n",
    "\n",
    "# rename columns\n",
    "clean_monster_df = clean_monster_df.rename(columns = {\n",
    "    \"cr\": \"challenge_rating\",\n",
    "    \"ac\": \"armor_class\",\n",
    "    \"hp\": \"hit_points\",\n",
    "    \"align\": \"alignment\"})\n",
    "\n",
    "# confirm data types\n",
    "clean_monster_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3de4e29",
   "metadata": {},
   "source": [
    "## Load\n",
    "\n",
    "Before we could load our data into a database, we had to create one and its constituent tables. To do this, we used [QuickDBD](https://app.quickdatabasediagrams.com/#/) to design the ERD then exported the schema as an SQL file (``schema.sql``). A graphical view of the ERD is available [here](erd.png). After creating the schema script we ran it within the pgAdmin4 Query Tool for the database we created.\n",
    "\n",
    "Once the data was properly formatted and the database was created, we were able to load the data into the PostgreSQL database as defined above. We first reflected the tables then stored these tables into their own objects. In order to transfer data to the database tables we had to create a SQLAlchemy ``Session`` then use the ``.add()`` method within an ``iterrow()`` loop. This process had to be ran twice; one for each DataFrame/table. Once the data was queued in the session we simply used the ``.commit()`` method to send the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838641fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the engine\n",
    "engine = create_engine(f\"postgresql+psycopg2://{postgresql_user}:{postgresql_key}@{postgresql_host}/{postgresql_db}\")\n",
    "\n",
    "# create the base reflector\n",
    "Base = automap_base()\n",
    "Base.prepare(engine, reflect = True)\n",
    "\n",
    "# define the tables\n",
    "dnd_monsters_tbl = Base.classes.dnd_monsters\n",
    "folklore_creatures_tbl = Base.classes.folklore_creatures\n",
    "\n",
    "# create the session\n",
    "session = Session(engine)\n",
    "\n",
    "# add data from the clean_monster_df to the current session\n",
    "for index, row in clean_monster_df.iterrows():\n",
    "    session.add(dnd_monsters_tbl(name = row[\"name\"], challenge_rating = row[\"challenge_rating\"], type = row[\"type\"], size = row[\"size\"], armor_class = row[\"armor_class\"], hit_points = row[\"hit_points\"], alignment = row[\"alignment\"]))\n",
    "\n",
    "# add data from the folklore_creatures_df to the current session\n",
    "for index, row in folklore_creatures_df.iterrows():\n",
    "    session.add(folklore_creatures_tbl(name = row[\"names\"], origin = row[\"origins\"], description = row[\"descriptions\"]))\n",
    "\n",
    "# send the new data to the database then flush the session\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce4952",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "To prove our data could be combined into one table for future analysis, we attempted an ``inner join`` on the ``name`` categories. This showed our data can be used to link folklore and mythological creatures with D&D statistics. We finally closed our session with the ``.close()`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3433a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the database for an inner join\n",
    "results = session.query(*[dnd_monsters_tbl.name, dnd_monsters_tbl.challenge_rating, dnd_monsters_tbl.armor_class, dnd_monsters_tbl.hit_points, folklore_creatures_tbl.description])\\\n",
    "                .join(folklore_creatures_tbl, dnd_monsters_tbl.name == folklore_creatures_tbl.name)\\\n",
    "                .order_by(dnd_monsters_tbl.name.asc()).all()\n",
    "\n",
    "# display the results as a collection of tuples\n",
    "for row in results:\n",
    "    print(row)\n",
    "\n",
    "# close and release session resource\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5965039f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
